{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Learning for Speech Emotion Recognition\n",
    "## Efthymios Tzinis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the appropriate modules \n",
    "import os, sys, glob\n",
    "import numpy as np\n",
    "sys.path.append('../')\n",
    "import config\n",
    "sys.path.append(config.BASE_PATH)\n",
    "from dataloader import fused_features_IEMOCAP as IEMOCAP_loader\n",
    "\n",
    "sys.path.append(config.PATTERN_SEARCH_MDS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session Folds Generator\n",
    "def get_dataset_in_one_array(features_dic,\n",
    "                             included_sessions=['Ses01', 'Ses02']):\n",
    "    speaker_indices = {}\n",
    "    x_all_list = []\n",
    "    Y_all = []\n",
    "    prev_ind = 0\n",
    "    for te_speaker, te_data in features_dic.items():  \n",
    "        ses_name = te_speaker[:-1]\n",
    "        if not ses_name in included_sessions:\n",
    "            continue\n",
    "        x_all_list.append(te_data['x'])\n",
    "        Y_all += te_data['y']\n",
    "        this_speaker_samples = len(te_data['y'])\n",
    "        \n",
    "        speaker_indices[te_speaker] = (prev_ind, prev_ind + this_speaker_samples)\n",
    "        prev_ind += this_speaker_samples\n",
    "        X_all = np.concatenate(x_all_list, axis=0)\n",
    "    return X_all, Y_all, speaker_indices, len(included_sessions)\n",
    "\n",
    "\n",
    "def generate_session_folds(X_all, Y_all, features_dic, speaker_indices):\n",
    "    sorted_speakers = sorted(speaker_indices)\n",
    "    for i in np.arange(0, len(sorted_speakers), 2):\n",
    "        sp1 = sorted_speakers[i]\n",
    "        sp2 = sorted_speakers[i+1]\n",
    "        \n",
    "        session_name = sp1[:-1]\n",
    "        \n",
    "        st1, et1 = speaker_indices[sp1]\n",
    "        st2, et2 = speaker_indices[sp2]\n",
    "        \n",
    "        Y_te = features_dic[sp1]['y'] + features_dic[sp2]['y']\n",
    "        X_te = np.concatenate([X_all[st1:et1, :], X_all[st2:et2, :]], axis=0)\n",
    "        \n",
    "        x_tr_list = []\n",
    "        Y_tr = []\n",
    "        for sp in sorted_speakers:\n",
    "            if sp == sp1 or sp == sp2:\n",
    "                continue\n",
    "            st, et = speaker_indices[sp] \n",
    "            x_tr_list.append(X_all[st:et, :])\n",
    "            Y_tr += features_dic[sp]['y']\n",
    "            \n",
    "        X_tr = np.concatenate(x_tr_list, axis=0)    \n",
    "        \n",
    "        yield session_name, X_te, Y_te, X_tr, Y_tr \n",
    "\n",
    "def generate_folds(features_dic,\n",
    "                   group_by = 'speaker'):\n",
    "    if group_by == 'speaker':\n",
    "        for te_speaker, te_data in features_dic.items():\n",
    "            x_tr_list = []\n",
    "            Y_tr = []\n",
    "            for tr_speaker, tr_data in features_dic.items():\n",
    "                if tr_speaker == te_speaker:\n",
    "                    continue\n",
    "                x_tr_list.append(tr_data['x'])\n",
    "                Y_tr += tr_data['y']\n",
    "\n",
    "            X_tr = np.concatenate(x_tr_list, axis=0)\n",
    "            yield te_speaker, te_data['x'], te_data['y'], X_tr, Y_tr\n",
    "     \n",
    "    elif group_by == 'session':\n",
    "        already_tested = []\n",
    "        for te_speaker, te_data in features_dic.items():\n",
    "            if not (te_speaker[:-1] in already_tested) :\n",
    "                already_tested.append(te_speaker[:-1])\n",
    "            else:\n",
    "                continue\n",
    "            X_val =  te_data['x']\n",
    "            Y_val = te_data['y']\n",
    "            x_tr_list = []\n",
    "            Y_tr = []\n",
    "            ses_name = te_speaker[:-1]\n",
    "            for tr_speaker, tr_data in features_dic.items():\n",
    "                if tr_speaker == te_speaker:\n",
    "                    continue\n",
    "                if tr_speaker[:-1] == ses_name:\n",
    "                    val_speaker = tr_speaker\n",
    "                    X_val = tr_data['x']\n",
    "                    Y_val = tr_data['y']\n",
    "                    continue\n",
    "                x_tr_list.append(tr_data['x'])\n",
    "                Y_tr += tr_data['y']\n",
    "\n",
    "            X_tr = np.concatenate(x_tr_list, axis=0)\n",
    "            X_ses = np.concatenate([te_data['x'], X_val], axis=0)\n",
    "            Y_ses = te_data['y'] + Y_val\n",
    "            yield ses_name, X_ses, Y_ses, X_tr, Y_tr\n",
    "            \n",
    "def fuse_excited_happiness(l):\n",
    "    return ['happy + excited' \n",
    "            if (e == 'excited' or e == 'happy') \n",
    "            else e for e in l ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all avaiulable Manifold Methods\n",
    "import multidimensional\n",
    "import multidimensional.common\n",
    "import multidimensional.mds \n",
    "import multidimensional.smacof\n",
    "from sklearn import manifold, decomposition\n",
    "\n",
    "class IdentityData(object):\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    \n",
    "    def fit_transform(self, x):\n",
    "        return x\n",
    "\n",
    "def get_manifold_methods(target_dim):\n",
    "    method_n_comp = 66\n",
    "    radius_barrier = 1e-3\n",
    "    explore_dim_percent = .9\n",
    "    starting_radius = 32\n",
    "    max_turns = 10000\n",
    "    point_filter = (multidimensional.point_filters.FixedStochasticFilter(keep_percent=1, recalculate_each=10))\n",
    "    radius_update = (multidimensional.radius_updates.AdaRadiusHalving(tolerance=.5*1e-3, burnout_tolerance=100000))\n",
    "\n",
    "    mds_obj = multidimensional.mds.MDS(target_dim, point_filter, radius_update, starting_radius=starting_radius, \n",
    "                                       radius_barrier=radius_barrier,\n",
    "                max_turns=max_turns, keep_history=False,\n",
    "                explore_dim_percent=explore_dim_percent)\n",
    "\n",
    "    manifold_methods = {\n",
    "        'Pattern Search MDS': { 'results': {}, 'object': multidimensional.mds.MDS(target_dim, point_filter, \n",
    "                                                         radius_update, starting_radius=starting_radius, \n",
    "                                                         radius_barrier=radius_barrier, max_turns=max_turns, \n",
    "                                                         keep_history=False,\n",
    "                                                         dissimilarities='precomputed',\n",
    "                                                         explore_dim_percent=explore_dim_percent)},\n",
    "        'MDS SMACOF': { 'results': {}, 'object': multidimensional.smacof.MDS(n_components=target_dim, n_init=1, \n",
    "                                                 max_iter=max_turns, dissimilarity='euclidean', n_jobs=8)},\n",
    "        'LTSA': { 'results': {}, 'object': manifold.LocallyLinearEmbedding(method_n_comp, target_dim, \n",
    "                                           eigen_solver='auto', method='ltsa',n_jobs=8)},\n",
    "        'Modified LLE': { 'results': {}, 'object': manifold.LocallyLinearEmbedding(method_n_comp, target_dim, \n",
    "                                           eigen_solver='auto', method='modified',n_jobs=8)},\n",
    "        'Hessian LLE': { 'results': {}, 'object': manifold.LocallyLinearEmbedding(method_n_comp, target_dim, \n",
    "                                           eigen_solver='auto', method='hessian',n_jobs=8)},\n",
    "        'LLE': { 'results': {}, 'object': manifold.LocallyLinearEmbedding(method_n_comp, target_dim, \n",
    "                                           eigen_solver='auto', method='standard',n_jobs=8)},\n",
    "        'Truncated SVD': { 'results': {}, 'object': decomposition.TruncatedSVD(n_components=target_dim)},\n",
    "        'Spectral Embedding': { 'results': {}, 'object': manifold.SpectralEmbedding(n_components=target_dim, \n",
    "                                                                                    n_jobs=8)},\n",
    "        'TSNE': { 'results': {}, 'object': manifold.TSNE(n_components=target_dim)},\n",
    "        'ISOMAP': { 'results': {}, 'object': manifold.Isomap(12, target_dim)},\n",
    "        'Original Data': { 'results': {}, 'object': IdentityData()}\n",
    "\n",
    "    }\n",
    "    return manifold_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the best performing nonlinear features for KNN classification after dimensionality reduction\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pprint \n",
    "import pandas as pd \n",
    "\n",
    "def run_IEMOCAP_session_KNN(n_neighbors, target_dims, methods_to_test, data_dic, included_sessions):\n",
    "\n",
    "    X_all, Y_all, speaker_indices, number_of_sessions = get_dataset_in_one_array(data_dic,\n",
    "                                                                                included_sessions=included_sessions)\n",
    "    df_results = {}\n",
    "    # normalize the input vectors \n",
    "    X_high = StandardScaler().fit_transform(X_all)\n",
    "    \n",
    "    print X_high.shape \n",
    "\n",
    "    for target_dim in target_dims:\n",
    "        manifold_methods = get_manifold_methods(target_dim)\n",
    "    #     methods_to_test = manifold_methods.keys()\n",
    "        methods_metrics = {}\n",
    "        for selected_method in methods_to_test:\n",
    "            metrics_l = {'uw_acc': dict([(k, 0.0) for k in n_neighbors]), 'w_acc': dict([(k, 0.0) for k in n_neighbors])}\n",
    "            print 'Checking Method: {}'.format(selected_method)\n",
    "            obj = manifold_methods[selected_method]['object']\n",
    "            \n",
    "            print 'Reducing Input from Dimension: {} to a Lower Embedded Manifold with dimensions: {}...'.format(\n",
    "                   X_high.shape[1], target_dim)\n",
    "            \n",
    "            try:\n",
    "                X_low = obj.fit_transform(X_high)\n",
    "            except Exception as e:\n",
    "                print e\n",
    "                methods_metrics[selected_method+' UA'] = metrics_l['uw_acc']\n",
    "                methods_metrics[selected_method+' WA'] = metrics_l['w_acc']\n",
    "                continue                \n",
    "\n",
    "            for k in n_neighbors:\n",
    "    #             print 'Testing for Nearest Neighbors: K={}'.format(k)\n",
    "                knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm='brute', leaf_size=30, \n",
    "                                           p=2, metric='minkowski', metric_params=None, n_jobs=8)\n",
    "\n",
    "                session_folds = generate_session_folds(X_low, Y_all, data_dic, speaker_indices)\n",
    "                for session, X_te, Y_te, X_tr, Y_tr in session_folds:\n",
    "    #                 print \"Testing for Session: {}\".format(session)\n",
    "                    Y_te, Y_tr = fuse_excited_happiness(Y_te), fuse_excited_happiness(Y_tr)\n",
    "                    \n",
    "                    try:\n",
    "                        knn.fit(X_tr, Y_tr) \n",
    "                        Y_predicted = knn.predict(X_te)\n",
    "\n",
    "                        w_acc = accuracy_score(Y_predicted, Y_te)\n",
    "                        cmat = confusion_matrix(Y_te, Y_predicted)\n",
    "                        with np.errstate(divide='ignore'):\n",
    "                            uw_acc = (cmat.diagonal() / (1.0 * cmat.sum(axis=1) + 1e-6)).mean()\n",
    "                            if np.isnan(uw_acc):\n",
    "                                uw_acc = 0.\n",
    "                        w_acc = round(w_acc*100,1)\n",
    "                        uw_acc = round(uw_acc*100,1)\n",
    "                        metrics_l['uw_acc'][k] += uw_acc/number_of_sessions\n",
    "                        metrics_l['w_acc'][k] += w_acc/number_of_sessions\n",
    "                    except:\n",
    "                        metrics_l['uw_acc'][k] += 0.\n",
    "                        metrics_l['w_acc'][k] += 0.\n",
    "    #             print 'Done'\n",
    "            methods_metrics[selected_method+' UA'] = metrics_l['uw_acc']\n",
    "            methods_metrics[selected_method+' WA'] = metrics_l['w_acc']\n",
    "#             pprint.pprint(metrics_l)\n",
    "\n",
    "        df = pd.DataFrame.from_dict(methods_metrics, orient=\"index\")\n",
    "        df_results[target_dim] = df[sorted(df.columns)]\n",
    "        \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define parameters for IEMOCAP Session Experiments \n",
    "n_neighbors = np.arange(1, 40, 4)\n",
    "target_dims = [2, 5, 10]\n",
    "\n",
    "# Find all appropriate files \n",
    "IEMOCAP_data_path = '/home/thymios/all_TRUE_IEMOCAP_feats/'\n",
    "l_feats_p = IEMOCAP_data_path + 'linear/IEMOCAP_linear_emobase2010'\n",
    "# nl_feats_l = glob.glob( IEMOCAP_data_path + '/utterance/*.dat')\n",
    "# nl_feats_p = nl_feats_l.pop()\n",
    "nl_feats_p = os.path.join(IEMOCAP_data_path, \n",
    "             'utterance/IEMOCAP-rqa-ad_hoc-tau-7-supremum-recurrence_rate-0.15-dur-0.03-fs-16000.dat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "included_sessions=['Ses01', 'Ses02', 'Ses03', 'Ses04', 'Ses05']\n",
    "methods_to_test = ['Truncated SVD', 'Spectral Embedding', \n",
    "                   'LLE', 'Hessian LLE', 'Modified LLE', 'LTSA']   \n",
    "# methods_to_test = ['Truncated SVD', 'Spectral Embedding']\n",
    "data_dic = IEMOCAP_loader.get_fused_features([l_feats_p, nl_feats_p])\n",
    "original_results = run_IEMOCAP_session_KNN(n_neighbors, [2014], ['Original Data'], data_dic, included_sessions)\n",
    "fused_results = run_IEMOCAP_session_KNN(n_neighbors, target_dims, methods_to_test, data_dic, included_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "for target_dim in sorted(original_results.keys()):\n",
    "    df = original_results[target_dim]\n",
    "    print \"Using Original Data\"\n",
    "    print display(df)\n",
    "\n",
    "for target_dim in sorted(fused_results.keys()):\n",
    "    df = fused_results[target_dim]\n",
    "    print \"For Target Dimension: {}\".format(target_dim)\n",
    "    print display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the experiment for EmoDB Speaker independent Experiments\n",
    "def get_dataset_for_all_speakers(features_dic):\n",
    "    speaker_indices = {}\n",
    "    x_all_list = []\n",
    "    Y_all = []\n",
    "    prev_ind = 0\n",
    "    for te_speaker, te_data in features_dic.items():  \n",
    "        x_all_list.append(te_data['x'])\n",
    "        Y_all += te_data['y']\n",
    "        this_speaker_samples = len(te_data['y'])\n",
    "        \n",
    "        speaker_indices[te_speaker] = (prev_ind, prev_ind + this_speaker_samples)\n",
    "        prev_ind += this_speaker_samples\n",
    "        X_all = np.concatenate(x_all_list, axis=0)\n",
    "    number_of_speakers = len(features_dic.keys())\n",
    "    return X_all, Y_all, speaker_indices, number_of_speakers\n",
    "\n",
    "def generate_speaker_independent_folds(X_all, Y_all, features_dic, speaker_indices):\n",
    "    sorted_speakers = sorted(speaker_indices.keys())\n",
    "    for (te_speaker, (st, et)) in speaker_indices.items():\n",
    "        Y_te = Y_all[st:et]\n",
    "        X_te = X_all[st:et, :]\n",
    "        \n",
    "        x_tr_list = []\n",
    "        Y_tr = []\n",
    "        for sp in sorted_speakers:\n",
    "            if sp == te_speaker:\n",
    "                continue\n",
    "            st, et = speaker_indices[sp] \n",
    "            x_tr_list.append(X_all[st:et, :])\n",
    "            Y_tr += Y_all[st:et]\n",
    "        X_tr = np.concatenate(x_tr_list, axis=0)    \n",
    "        \n",
    "        yield te_speaker, X_te, Y_te, X_tr, Y_tr \n",
    "\n",
    "\n",
    "def run_speaker_independent_KNN(n_neighbors, target_dims, methods_to_test, data_dic):\n",
    "\n",
    "    X_all, Y_all, speaker_indices, number_of_speakers = get_dataset_for_all_speakers(data_dic)\n",
    "    df_results = {}\n",
    "    # normalize the input vectors \n",
    "    X_high = StandardScaler().fit_transform(X_all)\n",
    "    \n",
    "    print X_high.shape \n",
    "\n",
    "    for target_dim in target_dims:\n",
    "        print \"Running for Target Dimensions={}\".format(target_dim)\n",
    "        manifold_methods = get_manifold_methods(target_dim)\n",
    "        methods_metrics = {}\n",
    "        for selected_method in methods_to_test:\n",
    "            metrics_l = {'uw_acc': dict([(k, 0.0) for k in n_neighbors]), \n",
    "                         'w_acc': dict([(k, 0.0) for k in n_neighbors])}\n",
    "            print 'Checking Method: {}'.format(selected_method)\n",
    "            try:\n",
    "                print 'Reducing Input from Dimension: {} to a Lower Embedded Manifold with dimensions: {}...'.format(\n",
    "                   X_high.shape[1], target_dim)\n",
    "                obj = manifold_methods[selected_method]['object']\n",
    "                if selected_method == 'Pattern Search MDS':\n",
    "                    d_goal = multidimensional.common.DISTANCE_MATRIX(X_high.astype(np.float64))\n",
    "                    X_low = obj.fit_transform(d_goal)\n",
    "                else:\n",
    "                    X_low = obj.fit_transform(X_high)\n",
    "\n",
    "            except:\n",
    "                methods_metrics[selected_method+' UA'] = metrics_l['uw_acc']\n",
    "                methods_metrics[selected_method+' WA'] = metrics_l['w_acc']\n",
    "                continue                    \n",
    "            \n",
    "            for k in n_neighbors:\n",
    "                knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm='brute', leaf_size=30, \n",
    "                                           p=2, metric='minkowski', metric_params=None, n_jobs=8)\n",
    "\n",
    "                speaker_folds = generate_speaker_independent_folds(X_low, Y_all, data_dic, speaker_indices)\n",
    "                for te_speaker, X_te, Y_te, X_tr, Y_tr in speaker_folds:                    \n",
    "                    try:\n",
    "                        knn.fit(X_tr, Y_tr) \n",
    "                        Y_predicted = knn.predict(X_te)\n",
    "\n",
    "                        w_acc = accuracy_score(Y_predicted, Y_te)\n",
    "                        cmat = confusion_matrix(Y_te, Y_predicted)\n",
    "                        with np.errstate(divide='ignore'):\n",
    "                            uw_acc = (cmat.diagonal() / (1.0 * cmat.sum(axis=1) + 1e-6)).mean()\n",
    "                            if np.isnan(uw_acc):\n",
    "                                uw_acc = 0.\n",
    "                        w_acc = round(w_acc*100,0)\n",
    "                        uw_acc = round(uw_acc*100,)\n",
    "                        metrics_l['uw_acc'][k] += uw_acc/number_of_speakers\n",
    "                        metrics_l['w_acc'][k] += w_acc/number_of_speakers\n",
    "                    except:\n",
    "                        metrics_l['uw_acc'][k] += 0.\n",
    "                        metrics_l['w_acc'][k] += 0.\n",
    "                    \n",
    "            methods_metrics[selected_method+' UA'] = metrics_l['uw_acc']\n",
    "            methods_metrics[selected_method+' WA'] = metrics_l['w_acc']\n",
    "\n",
    "        df = pd.DataFrame.from_dict(methods_metrics, orient=\"index\")\n",
    "        df_results[target_dim] = df[sorted(df.columns)]\n",
    "        \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for Speaker Independent Experiments \n",
    "n_neighbors = np.arange(1, 40, 4)\n",
    "target_dims = [2, 5, 10, 25]\n",
    "\n",
    "# Find all appropriate files \n",
    "data_path = '/home/thymios/all_BERLIN_features/'\n",
    "berlin_l_feats_p = data_path + 'linear/BERLIN_linear_emobase2010'\n",
    "# nl_feats_l = glob.glob( IEMOCAP_data_path + '/utterance/*.dat')\n",
    "# nl_feats_p = nl_feats_l.pop()\n",
    "berlin_nl_feats_p = os.path.join(data_path, \n",
    "             'rqa/utterance/BERLIN-rqa-ad_hoc-tau-7-manhattan-recurrence_rate-0.15-dur-0.02-fs-16000.dat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_to_test = ['Pattern Search MDS', 'MDS SMACOF','Truncated SVD', 'Spectral Embedding', 'LLE', \n",
    "                   'Hessian LLE', 'Modified LLE', 'LTSA', 'ISOMAP']   \n",
    "# methods_to_test = ['Pattern Search MDS']\n",
    "# methods_to_test = ['Truncated SVD']\n",
    "\n",
    "data_dic = IEMOCAP_loader.get_fused_features([berlin_nl_feats_p])\n",
    "berlin_original_nl_results = run_speaker_independent_KNN(n_neighbors, [2014], ['Original Data'], data_dic)\n",
    "berlin_nl_results = run_speaker_independent_KNN(n_neighbors, target_dims, methods_to_test, data_dic)\n",
    "\n",
    "data_dic = IEMOCAP_loader.get_fused_features([berlin_l_feats_p, berlin_nl_feats_p])\n",
    "berlin_original_fused_results = run_speaker_independent_KNN(n_neighbors, [2014], ['Original Data'], data_dic)\n",
    "berlin_fused_results = run_speaker_independent_KNN(n_neighbors, target_dims, methods_to_test, data_dic)\n",
    "\n",
    "data_dic = IEMOCAP_loader.get_fused_features([berlin_l_feats_p])\n",
    "berlin_original_l_results = run_speaker_independent_KNN(n_neighbors, [2014], ['Original Data'], data_dic)\n",
    "berlin_l_results = run_speaker_independent_KNN(n_neighbors, target_dims, methods_to_test, data_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "print \"Using RQA Feature Set and Dimensionality Reduction...\"\n",
    "\n",
    "def latex_preformat_print(df):\n",
    "    methods = {}\n",
    "    for ind in df.index.values:\n",
    "        if not ind[:-3] in methods and ind[-2:] == 'WA':\n",
    "            methods[ind[:-3]] = list(df[[1,5,9,13,17,21]].loc[ind])\n",
    "    for ind in df.index.values:\n",
    "        if ind[-2:] == 'UA':\n",
    "            methods[ind[:-3]] += list(df[[1,5,9,13,17,21]].loc[ind])\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(methods, orient=\"index\")\n",
    "    print df.to_latex()\n",
    "\n",
    "for target_dim in sorted(berlin_original_nl_results.keys()):\n",
    "    df = berlin_original_nl_results[target_dim]\n",
    "    print \"Using Original Data\"\n",
    "    print display(df)\n",
    "    latex_preformat_print(df)\n",
    "    \n",
    "for target_dim in sorted(berlin_nl_results.keys()):\n",
    "    df = berlin_nl_results[target_dim]\n",
    "    print \"For Target Dimension: {}\".format(target_dim)\n",
    "    print display(df)\n",
    "    latex_preformat_print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "print \"Using Emobase Features and Dimensionality Reduction...\"\n",
    "\n",
    "for target_dim in sorted(berlin_original_l_results.keys()):\n",
    "    df = berlin_original_l_results[target_dim]\n",
    "    print \"Using Original Data\"\n",
    "    print display(df)\n",
    "    latex_preformat_print(df)\n",
    "\n",
    "for target_dim in sorted(berlin_l_results.keys()):\n",
    "    df = berlin_l_results[target_dim]\n",
    "    print \"For Target Dimension: {}\".format(target_dim)\n",
    "    print display(df)\n",
    "    latex_preformat_print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "print \"Using Fused Features and Dimensionality Reduction...\"\n",
    "\n",
    "for target_dim in sorted(berlin_original_fused_results.keys()):\n",
    "    df = berlin_original_fused_results[target_dim]\n",
    "    print \"Using Original Data\"\n",
    "    print display(df)\n",
    "    latex_preformat_print(df)\n",
    "\n",
    "for target_dim in sorted(berlin_fused_results.keys()):\n",
    "    df = berlin_fused_results[target_dim]\n",
    "    print \"For Target Dimension: {}\".format(target_dim)\n",
    "    print display(df)\n",
    "    latex_preformat_print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
